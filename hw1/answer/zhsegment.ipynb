{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from zhsegment import *\n",
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签 订 高 科 技 合 作 协 议\n",
      "新 华 社 上 海 八 月 三 十 一 日 电 （ 记 者 白 国 良 、 夏 儒 阁 ）\n",
      "“ 中 美 合 作 高 科 技 项 目 签 字 仪 式 ” 今 天 在 上 海 举 行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip(), default=True))\n",
    "        output_full.append(output)\n",
    "        \n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hw1 path to be able to import \n",
    "hw1_path = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.append(hw1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.27\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the iterative algorithm, we follow these steps:\n",
    "1. Initialize heap with segments at position 0 (with valid words and Unknown words)\n",
    "2. Iteratively fill the memoization table by:\n",
    "       - popping values from the heap\n",
    "       - calculating probability of the segmented sentence as probability(previous word) + probability(current word)\n",
    "       - storing the new probability in the table only if it is higher than the already stored one\n",
    "       - populating the heap with the rest of the new words, which do not already exist in the heap\n",
    "3. Reconstruct the final segmentation by following the back pointer of the last entry in the memoization table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the unigram model on english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to break up in 5 words\n",
      "what makes god smile\n",
      "10 peopl e who mean alot to me\n",
      "worst day in 4 words\n",
      "love story in 5 words\n",
      "top 3 fav our it e comic s\n",
      "10 break up lines\n",
      "thing s that make you smile\n",
      "best fema leath let e\n",
      "worst boss in 5 words\n",
      "now is the time for all good\n",
      "it is a truth univ er sally ac knowl edged\n",
      "when in the cours e of human event sit be comes neces sary\n",
      "it was ab right cold day in april and the clock s were stri king thir teen\n",
      "it was the best of times it was the worst of times it was the age of w is dom it was the age of fool ish ness\n",
      "as greg or samsa awoke one morn ing from un easy dream she found him self trans for med in his bed into a gig antic in sect\n",
      "in a hole in the g round there lived a hobb it not a nasty dirty wet hole fill ed with the ends of worms and an oozy smell nor yet a dry bare sandy hole with no thing in it to sit down on or to eat it was a hobb it hole and that means com fort\n",
      "far out in the un chart ed back water s of the unf as hi on able end of the we stern spir alarm of the galax y lies a small unre garde dye l low sun\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../../hw0/ensegment/data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * 1000**(len(k)))))\n",
    "segmenter = Segment(Pw)\n",
    "output_full = []\n",
    "with open(\"../../hw0/ensegment/data/input/test.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip(), default=False))\n",
    "        output_full.append(output)\n",
    "\n",
    "print(\"\\n\".join(output_full)) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the unigram model on dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.93\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * 1000**(len(k)))))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip(), default=False))\n",
    "        output_full.append(output)\n",
    "        \n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution using Bigram model, along with Jelinek-Mercer smoothing\n",
    "To make our model more generalizable for test data, we implement the combination of Bigram and Unigram models, along with Jelinek-Mercer smoothing next.\n",
    "For this, we use the given word counts for Chinese Bigrams count_2w.txt to determine the Conditional probability and populate a dictionary as per the following formula:<br><br>\n",
    "PML(Wi | Wi−1) = c(Wi−1, Wi) / c(Wi-1)<br><br>\n",
    "where c(Wi−1, Wi) is the count of the bigram (Wi−1, Wi) in the Bigram text file, and c(Wi-1) is the count of the unigram (Wi-1).<br><br>\n",
    "Then we apply Jelinek-Mercer Smoothing as follows:<br><br>\n",
    "PJM(wi | wi−1) = lambda * PML(wi | wi−1) + (1 − lambda) * PML(wi)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the bigram model on dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.93\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * 1000**(len(k)))))\n",
    "Pwc = Pdist_cond(Pw, data=datafile(\"../data/count_2w.txt\", mode='bigram'))\n",
    "segmenter = Segment(Pw, Pwc, lambda_bigram=0.01) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip(), default=False))\n",
    "        output_full.append(output)\n",
    "        \n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we try to selcet a subset of data from 1 million Chinese datasets to create a second  dataset for our evaluation besides the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the 1M chinsese sentences dataset\n",
    "import numpy as np\n",
    "num_lines = 350\n",
    "random_lines = set(np.random.choice(range(1000000), num_lines, replace=False))\n",
    "million_sent_path = \"../data/wseg_simplified_cn.txt\"\n",
    "\n",
    "subset_lines = []\n",
    "with open(million_sent_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i in random_lines:\n",
    "            subset_lines.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the distibution of the sentence lengths for test dataset and the dataset we created.\n",
    "#### First one is for test data and second one is for the one we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([104., 112.,  70.,  34.,  18.,   7.,   3.,   0.,   1.,   1.]),\n",
       " array([  4. ,  19.9,  35.8,  51.7,  67.6,  83.5,  99.4, 115.3, 131.2,\n",
       "        147.1, 163. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 1295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making sure the test data and the sample data have similar distribuations in terms of sentence length. \n",
    "sent_length = []\n",
    "with open('../data/input/test.txt') as f:\n",
    "    for line in f:\n",
    "        sent_length.append(len(line))\n",
    "plt.hist(sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([105., 123.,  75.,  35.,   5.,   5.,   1.,   0.,   0.,   1.]),\n",
       " array([  3. ,  39.9,  76.8, 113.7, 150.6, 187.5, 224.4, 261.3, 298.2,\n",
       "        335.1, 372. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 1296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_length = []\n",
    "for line in subset_lines:\n",
    "    sent_length.append(len(line))\n",
    "plt.hist(sent_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample sentences from the dataset we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['警方 立即 将 薛 文祥 与 薛 章和 列 入 侦查 重点 。\\n',\n",
       " '2695×1559×1542mm 的 车身 尺寸 几乎 是 目 前 量 产 车 中 最小的 一 款 了。\\n',\n",
       " '今天 呢 ， 我们 就 来 关注 这一 事件 。\\n',\n",
       " '所以 适当的 时候 呢 ， 还 会 有 类似 歼十 这样 ，\\n',\n",
       " '虽然 被掐头去尾 , 但 是 可以确定 是 同 一部 短 片 .\\n']"
      ]
     },
     "execution_count": 1297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next two cells will push the sentences into a file on which we evaluate our results on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write this subset to a new reference file\n",
    "million_sent_subset_path = \"../data/reference/million_sent_sample.out\"\n",
    "with open(million_sent_subset_path, 'w') as f:\n",
    "    for line in subset_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the spaces in the selected lines and use this as an extra input for the model to evaluate\n",
    "million_sent_subset_input_path = \"../data/input/million_sent_sample.txt\"\n",
    "with open(million_sent_subset_input_path, 'w') as f:\n",
    "    for line in subset_lines:\n",
    "        f.write(''.join(line.split(' ')))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are some helper functions that we use for the grid search. The grid search is used for choosing lambda values for the smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for choosing the right lambda and delta parameters\n",
    "def run_input_file(path):\n",
    "    results = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            output = \" \".join(segmenter.segment(line.strip(), default=False))\n",
    "            results.append(output)\n",
    "    return results\n",
    "\n",
    "def evaluate_input_file(path, lines):\n",
    "    with open(path, 'r') as refh:\n",
    "        ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "        tally = fscore(ref_data, lines)\n",
    "        return tally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next two cells will create the file the count of triple words. This is used for the trigram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of all the trigrams\n",
    "Pwc2 = {}\n",
    "with open('../data/train.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        tokens = ['<S>'] + tokens\n",
    "        for i in range(2, len(tokens)):\n",
    "            if (tokens[i-2], tokens[i-1], tokens[i]) in Pwc2:\n",
    "                Pwc2[(tokens[i-2], tokens[i-1], tokens[i])] += 1\n",
    "            else:\n",
    "                Pwc2[(tokens[i-2], tokens[i-1], tokens[i])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results of trigram counts into a file\n",
    "with open('../data/count_3w.txt', 'w') as f:\n",
    "    for k, v in Pwc2.items():\n",
    "        f.write(k[0] + ' ' + k[1] + ' ' + k[2] + ' {}'.format(v) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution using trigram model, along with Jelinek-Mercer smoothing\n",
    "We apply Jelinek-Mercer Smoothing, similar to our previous Bigram model, as follows:<br><br>\n",
    "PJM(wi | wi−1, wi-2) = lambda1 * PML(wi | wi−1, wi-2) + (1 − lambda1) * { lambda2 * PML(wi | wi-1) + (1 - lambda2) * PML(wi) }<br><br>\n",
    "We apply this while calculating probability for a segment with respect to the previous two segments in the input sequence and we make use of the trigram, bigram and the unigram word counts for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Adding trigram\n",
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * 1000**(len(k)))))\n",
    "Pwc = Pdist_cond(Pw, data=datafile(\"../data/count_2w.txt\", mode='bigram'))\n",
    "Pwc2 = Pdist_cond_tri(Pwc, data=datafile(\"../data/count_3w.txt\", mode='trigram'))\n",
    "segmenter = Segment(Pw, Pwc, Pwc2, lambda_bigram=0.01, lambda_trigram=0.01) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip(), default=False))\n",
    "        output_full.append(output)\n",
    "\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now use a grid search to find best values of lambda for the unigram, bigram and the trigram model based on the score on dev and the sample data we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1604,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lambda_bigram 0, lambda_trigram 0 and delta 1000 ...\n",
      "dev score: 0.9264056733482885 and sample data score: 0.7862988384371701\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0 and delta 5000 ...\n",
      "dev score: 0.9269485188380788 and sample data score: 0.7984057598354333\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0 and delta 10000 ...\n",
      "dev score: 0.9260907003444316 and sample data score: 0.7956851981872726\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0 and delta 20000 ...\n",
      "dev score: 0.9176840151352895 and sample data score: 0.7936911727295853\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.1 and delta 1000 ...\n",
      "dev score: 0.9243563783627423 and sample data score: 0.7880309135345795\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.1 and delta 5000 ...\n",
      "dev score: 0.92517789118091 and sample data score: 0.7990741336076641\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.1 and delta 10000 ...\n",
      "dev score: 0.9244010902309568 and sample data score: 0.7960879570442341\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.1 and delta 20000 ...\n",
      "dev score: 0.9160185515519087 and sample data score: 0.7942504611079311\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.4 and delta 1000 ...\n",
      "dev score: 0.9216482878699942 and sample data score: 0.7808913797704831\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.4 and delta 5000 ...\n",
      "dev score: 0.9247141727187748 and sample data score: 0.797523060052893\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.4 and delta 10000 ...\n",
      "dev score: 0.9245567439523366 and sample data score: 0.7971776779987171\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.4 and delta 20000 ...\n",
      "dev score: 0.9234737747205503 and sample data score: 0.7955110629343876\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.8 and delta 1000 ...\n",
      "dev score: 0.9126768824693894 and sample data score: 0.7558163334925291\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.8 and delta 5000 ...\n",
      "dev score: 0.9242993354521815 and sample data score: 0.7881974107905632\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.8 and delta 10000 ...\n",
      "dev score: 0.9252699784017279 and sample data score: 0.7926094003241491\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.8 and delta 20000 ...\n",
      "dev score: 0.9250341407316898 and sample data score: 0.798326359832636\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.9 and delta 1000 ...\n",
      "dev score: 0.9058243133789853 and sample data score: 0.7398227637773469\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.9 and delta 5000 ...\n",
      "dev score: 0.9200087025890202 and sample data score: 0.7770072019205122\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.9 and delta 10000 ...\n",
      "dev score: 0.923898916967509 and sample data score: 0.7877395641900761\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.9 and delta 20000 ...\n",
      "dev score: 0.9250701792269489 and sample data score: 0.7927414128321452\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.95 and delta 1000 ...\n",
      "dev score: 0.8905076934512749 and sample data score: 0.7124215278267616\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.95 and delta 5000 ...\n",
      "dev score: 0.9136937331290582 and sample data score: 0.7656102494942685\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.95 and delta 10000 ...\n",
      "dev score: 0.9199768048709771 and sample data score: 0.7764314247669775\n",
      "----------\n",
      "Testing lambda_bigram 0, lambda_trigram 0.95 and delta 20000 ...\n",
      "dev score: 0.9231546287610939 and sample data score: 0.7877197585935449\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0 and delta 1000 ...\n",
      "dev score: 0.9262472885032537 and sample data score: 0.7918995331711487\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0 and delta 5000 ...\n",
      "dev score: 0.9258753325185132 and sample data score: 0.7986383197379406\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0 and delta 10000 ...\n",
      "dev score: 0.9246771879483501 and sample data score: 0.7960892069780817\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0 and delta 20000 ...\n",
      "dev score: 0.9162920145579105 and sample data score: 0.7941494435612083\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 1000 ...\n",
      "dev score: 0.9252045471001376 and sample data score: 0.7887268167117681\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 5000 ...\n",
      "dev score: 0.9255770475300209 and sample data score: 0.7992027261621552\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 10000 ...\n",
      "dev score: 0.9246663796814465 and sample data score: 0.7968020466901183\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 20000 ...\n",
      "dev score: 0.9199656995855366 and sample data score: 0.7953908836261777\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.4 and delta 1000 ...\n",
      "dev score: 0.9209437386569873 and sample data score: 0.782770787564077\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.4 and delta 5000 ...\n",
      "dev score: 0.9242718446601943 and sample data score: 0.7974479603016047\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.4 and delta 10000 ...\n",
      "dev score: 0.9240379092475588 and sample data score: 0.7970233512958685\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.4 and delta 20000 ...\n",
      "dev score: 0.9226250179108755 and sample data score: 0.7950009564496588\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.8 and delta 1000 ...\n",
      "dev score: 0.9118745879422753 and sample data score: 0.7568119891008175\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.8 and delta 5000 ...\n",
      "dev score: 0.9248880543117146 and sample data score: 0.7913254275044226\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.8 and delta 10000 ...\n",
      "dev score: 0.9249046007631938 and sample data score: 0.7944850799404493\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.8 and delta 20000 ...\n",
      "dev score: 0.9247466398332496 and sample data score: 0.7977882080627532\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.9 and delta 1000 ...\n",
      "dev score: 0.9049827040553471 and sample data score: 0.740750966316952\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.9 and delta 5000 ...\n",
      "dev score: 0.9207454136755856 and sample data score: 0.7792777814723683\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.9 and delta 10000 ...\n",
      "dev score: 0.9249873655331744 and sample data score: 0.7910692070974923\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.9 and delta 20000 ...\n",
      "dev score: 0.9247048661099914 and sample data score: 0.7945400439901669\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.95 and delta 1000 ...\n",
      "dev score: 0.8943270300333704 and sample data score: 0.7175057931325047\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.95 and delta 5000 ...\n",
      "dev score: 0.9154642180440169 and sample data score: 0.7658313123147399\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.95 and delta 10000 ...\n",
      "dev score: 0.9209248387330579 and sample data score: 0.7790396493325363\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.95 and delta 20000 ...\n",
      "dev score: 0.9242424242424242 and sample data score: 0.7905789990186457\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0 and delta 1000 ...\n",
      "dev score: 0.9238578680203047 and sample data score: 0.7871326449563145\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0 and delta 5000 ...\n",
      "dev score: 0.9253001653605579 and sample data score: 0.798352320267748\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0 and delta 10000 ...\n",
      "dev score: 0.9249318214439501 and sample data score: 0.7973085549503365\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0 and delta 20000 ...\n",
      "dev score: 0.923859322398109 and sample data score: 0.7956920723935763\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.1 and delta 1000 ...\n",
      "dev score: 0.9215017064846416 and sample data score: 0.7842642961390475\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.1 and delta 5000 ...\n",
      "dev score: 0.9246368474039983 and sample data score: 0.7973721499420328\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.1 and delta 10000 ...\n",
      "dev score: 0.9241923905240488 and sample data score: 0.7973839446011798\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.1 and delta 20000 ...\n",
      "dev score: 0.923484739934088 and sample data score: 0.7950532287881685\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lambda_bigram 0.4, lambda_trigram 0.4 and delta 1000 ...\n",
      "dev score: 0.9198078882258768 and sample data score: 0.7792744731928515\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.4 and delta 5000 ...\n",
      "dev score: 0.9259579371938922 and sample data score: 0.7953632949099857\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.4 and delta 10000 ...\n",
      "dev score: 0.9246801782377462 and sample data score: 0.7979681069958847\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.4 and delta 20000 ...\n",
      "dev score: 0.9236285406955899 and sample data score: 0.7960058887537604\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.8 and delta 1000 ...\n",
      "dev score: 0.910943340907425 and sample data score: 0.7553278688524591\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.8 and delta 5000 ...\n",
      "dev score: 0.9219745222929938 and sample data score: 0.7854080084438286\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.8 and delta 10000 ...\n",
      "dev score: 0.9247404844290659 and sample data score: 0.7950019523623584\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.8 and delta 20000 ...\n",
      "dev score: 0.9242162784009205 and sample data score: 0.7968014445089314\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.9 and delta 1000 ...\n",
      "dev score: 0.9014313117898776 and sample data score: 0.7332916926920674\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.9 and delta 5000 ...\n",
      "dev score: 0.918101882130659 and sample data score: 0.7764360531056108\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.9 and delta 10000 ...\n",
      "dev score: 0.9221418234442836 and sample data score: 0.7851637130245736\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.9 and delta 20000 ...\n",
      "dev score: 0.9245514160121063 and sample data score: 0.795552087397581\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.95 and delta 1000 ...\n",
      "dev score: 0.8871003800581265 and sample data score: 0.7047511312217194\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.95 and delta 5000 ...\n",
      "dev score: 0.9116809116809117 and sample data score: 0.7602623926421858\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.95 and delta 10000 ...\n",
      "dev score: 0.9185734001598024 and sample data score: 0.77572208172501\n",
      "----------\n",
      "Testing lambda_bigram 0.4, lambda_trigram 0.95 and delta 20000 ...\n",
      "dev score: 0.9221418234442836 and sample data score: 0.7852348993288591\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0 and delta 1000 ...\n",
      "dev score: 0.9164719626168225 and sample data score: 0.7697890068539175\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0 and delta 5000 ...\n",
      "dev score: 0.924864669794298 and sample data score: 0.7910554794967078\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0 and delta 10000 ...\n",
      "dev score: 0.9251025253615368 and sample data score: 0.7931034482758621\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0 and delta 20000 ...\n",
      "dev score: 0.9244252873563218 and sample data score: 0.796120246659815\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.1 and delta 1000 ...\n",
      "dev score: 0.9144693594332043 and sample data score: 0.7660921088068947\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.1 and delta 5000 ...\n",
      "dev score: 0.9243539771907031 and sample data score: 0.7908859437226611\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.1 and delta 10000 ...\n",
      "dev score: 0.9243056554899985 and sample data score: 0.7927065821802664\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.1 and delta 20000 ...\n",
      "dev score: 0.9239278787443431 and sample data score: 0.795810845540992\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.4 and delta 1000 ...\n",
      "dev score: 0.9116980027800131 and sample data score: 0.7604562737642586\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.4 and delta 5000 ...\n",
      "dev score: 0.9212296564195299 and sample data score: 0.7872046337128942\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.4 and delta 10000 ...\n",
      "dev score: 0.9241627655743607 and sample data score: 0.7952136307472198\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.4 and delta 20000 ...\n",
      "dev score: 0.9239060142272041 and sample data score: 0.7971631205673759\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.8 and delta 1000 ...\n",
      "dev score: 0.8966077895203606 and sample data score: 0.7285266457680251\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.8 and delta 5000 ...\n",
      "dev score: 0.9144272085062995 and sample data score: 0.7720494817786692\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.8 and delta 10000 ...\n",
      "dev score: 0.9190872872147772 and sample data score: 0.7827007009654807\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.8 and delta 20000 ...\n",
      "dev score: 0.9228214569345174 and sample data score: 0.7895872849761265\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.9 and delta 1000 ...\n",
      "dev score: 0.883863263173608 and sample data score: 0.6894736842105263\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.9 and delta 5000 ...\n",
      "dev score: 0.9107521380016081 and sample data score: 0.7602378699824301\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.9 and delta 10000 ...\n",
      "dev score: 0.915084042785418 and sample data score: 0.7737928270887597\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.9 and delta 20000 ...\n",
      "dev score: 0.9188092996306222 and sample data score: 0.7824881141045958\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.95 and delta 1000 ...\n",
      "dev score: 0.8699254349627175 and sample data score: 0.6631785480706345\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.95 and delta 5000 ...\n",
      "dev score: 0.903918253326472 and sample data score: 0.7496067300458246\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.95 and delta 10000 ...\n",
      "dev score: 0.9107898963352316 and sample data score: 0.7610822481613926\n",
      "----------\n",
      "Testing lambda_bigram 0.8, lambda_trigram 0.95 and delta 20000 ...\n",
      "dev score: 0.915806310891377 and sample data score: 0.7735660515621878\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0 and delta 1000 ...\n",
      "dev score: 0.9121240945342798 and sample data score: 0.7611111111111111\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0 and delta 5000 ...\n",
      "dev score: 0.9202809354862066 and sample data score: 0.783760627430304\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0 and delta 10000 ...\n",
      "dev score: 0.9239091236927516 and sample data score: 0.7907764460656592\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0 and delta 20000 ...\n",
      "dev score: 0.924482163406214 and sample data score: 0.7931568754034861\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.1 and delta 1000 ...\n",
      "dev score: 0.9095304373305985 and sample data score: 0.7548659316727916\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.1 and delta 5000 ...\n",
      "dev score: 0.9201129861664372 and sample data score: 0.783457555570213\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.1 and delta 10000 ...\n",
      "dev score: 0.9241090751695281 and sample data score: 0.7906855391037766\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.1 and delta 20000 ...\n",
      "dev score: 0.9238958423248453 and sample data score: 0.792684503037353\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.4 and delta 1000 ...\n",
      "dev score: 0.9045307443365697 and sample data score: 0.743053645116919\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.4 and delta 5000 ...\n",
      "dev score: 0.9175137961080453 and sample data score: 0.7818531538104397\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.4 and delta 10000 ...\n",
      "dev score: 0.9210412147505423 and sample data score: 0.7866999605730055\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.4 and delta 20000 ...\n",
      "dev score: 0.9238300935925126 and sample data score: 0.7953999090377495\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.8 and delta 1000 ...\n",
      "dev score: 0.8848647440196736 and sample data score: 0.6925368898978433\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.8 and delta 5000 ...\n",
      "dev score: 0.9117861837301007 and sample data score: 0.7648839556004037\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.8 and delta 10000 ...\n",
      "dev score: 0.9142233539468899 and sample data score: 0.7758149456702887\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.8 and delta 20000 ...\n",
      "dev score: 0.9185979142526071 and sample data score: 0.7832684568186317\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lambda_bigram 0.9, lambda_trigram 0.9 and delta 1000 ...\n",
      "dev score: 0.872049315892347 and sample data score: 0.6659908768373036\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.9 and delta 5000 ...\n",
      "dev score: 0.9048458149779735 and sample data score: 0.7519094380796508\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.9 and delta 10000 ...\n",
      "dev score: 0.9111208406304729 and sample data score: 0.7658831430490263\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.9 and delta 20000 ...\n",
      "dev score: 0.9152493094926588 and sample data score: 0.7754423307170414\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.95 and delta 1000 ...\n",
      "dev score: 0.8630033303057826 and sample data score: 0.6465669014084507\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.95 and delta 5000 ...\n",
      "dev score: 0.8992556562753334 and sample data score: 0.7330473290893994\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.95 and delta 10000 ...\n",
      "dev score: 0.9046501393574886 and sample data score: 0.7533632286995516\n",
      "----------\n",
      "Testing lambda_bigram 0.9, lambda_trigram 0.95 and delta 20000 ...\n",
      "dev score: 0.9116489284152208 and sample data score: 0.7663313212608986\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0 and delta 1000 ...\n",
      "dev score: 0.9029834254143647 and sample data score: 0.7410720429367647\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0 and delta 5000 ...\n",
      "dev score: 0.915781818181818 and sample data score: 0.776595744680851\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0 and delta 10000 ...\n",
      "dev score: 0.9200260548599553 and sample data score: 0.7832784726793943\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0 and delta 20000 ...\n",
      "dev score: 0.9231656335591755 and sample data score: 0.7907521979811136\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.1 and delta 1000 ...\n",
      "dev score: 0.9025414364640884 and sample data score: 0.7381559892421212\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.1 and delta 5000 ...\n",
      "dev score: 0.9150963986904328 and sample data score: 0.7754640409819706\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.1 and delta 10000 ...\n",
      "dev score: 0.9199362965107861 and sample data score: 0.7832158619326791\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.1 and delta 20000 ...\n",
      "dev score: 0.9233652944993151 and sample data score: 0.7902731599191603\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.4 and delta 1000 ...\n",
      "dev score: 0.8940549344784186 and sample data score: 0.7200669829751606\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.4 and delta 5000 ...\n",
      "dev score: 0.9121399927087132 and sample data score: 0.7688701359587435\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.4 and delta 10000 ...\n",
      "dev score: 0.917507073931655 and sample data score: 0.7817977379456313\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.4 and delta 20000 ...\n",
      "dev score: 0.9210526315789475 and sample data score: 0.7866614152553499\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.8 and delta 1000 ...\n",
      "dev score: 0.8734424260621528 and sample data score: 0.6690704550371385\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.8 and delta 5000 ...\n",
      "dev score: 0.9049295774647887 and sample data score: 0.7551810831011755\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.8 and delta 10000 ...\n",
      "dev score: 0.9103508643956525 and sample data score: 0.7655547352171287\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.8 and delta 20000 ...\n",
      "dev score: 0.9144559924413113 and sample data score: 0.7751927678808828\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.9 and delta 1000 ...\n",
      "dev score: 0.8647423303611909 and sample data score: 0.6499890278692123\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.9 and delta 5000 ...\n",
      "dev score: 0.8992190953293061 and sample data score: 0.7351194586383096\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.9 and delta 10000 ...\n",
      "dev score: 0.9053202403634766 and sample data score: 0.7568922305764412\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.9 and delta 20000 ...\n",
      "dev score: 0.9105619943144544 and sample data score: 0.766704644460827\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.95 and delta 1000 ...\n",
      "dev score: 0.856469873109946 and sample data score: 0.630495928941525\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.95 and delta 5000 ...\n",
      "dev score: 0.8868428868428869 and sample data score: 0.7017766497461929\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.95 and delta 10000 ...\n",
      "dev score: 0.8997054491899853 and sample data score: 0.7380363036303631\n",
      "----------\n",
      "Testing lambda_bigram 0.95, lambda_trigram 0.95 and delta 20000 ...\n",
      "dev score: 0.905549860887392 and sample data score: 0.7580470651879903\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Using grid search to find the best parameters\n",
    "deltas = [1000, 5000, 10000, 20000]\n",
    "lambda_bigram_vals = [0, 0.1, 0.4, 0.8, 0.9, 0.95]\n",
    "lambda_trigram_vals = [0, 0.1, 0.4, 0.8, 0.9, 0.95]\n",
    "\n",
    "for lambda_bigram in lambda_bigram_vals:\n",
    "    for lambda_trigram in lambda_trigram_vals:\n",
    "        for delta in deltas:\n",
    "            Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * delta**(len(k)))))\n",
    "            Pwc = Pdist_cond(Pw, data=datafile(\"../data/count_2w.txt\", mode='bigram'))\n",
    "            Pwc2 = Pdist_cond_tri(Pwc, data=datafile(\"../data/count_3w.txt\", mode='trigram'))\n",
    "            segmenter = Segment(Pw, Pwc, Pwc2, lambda_bigram=lambda_bigram, lambda_trigram=lambda_trigram)\n",
    "            # evaluate results on dev\n",
    "            output_full = run_input_file(\"../data/input/dev.txt\")\n",
    "            tally_dev = evaluate_input_file('../data/reference/dev.out', output_full)\n",
    "            # evaluate results on the sample sentences from 1 Million Chinese sentences\n",
    "            output_full = run_input_file(\"../data/input/million_sent_sample.txt\")\n",
    "            tally_sample = evaluate_input_file('../data/reference/million_sent_sample.out', output_full)\n",
    "            print('Testing lambda_bigram {}, lambda_trigram {} and delta {} ...'.format(lambda_bigram, lambda_trigram, delta))\n",
    "            print('dev score: {} and sample data score: {}'.format(tally_dev, tally_sample))\n",
    "            print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By naroowing down the search in the next two cells we find the best lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 3700 ...\n",
      "dev score: 0.9254010502841523 and sample data score: 0.7987891279144661\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 4000 ...\n",
      "dev score: 0.9254997842657846 and sample data score: 0.7995108450794877\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 4300 ...\n",
      "dev score: 0.9254997842657846 and sample data score: 0.7992536353107708\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 4600 ...\n",
      "dev score: 0.9254997842657846 and sample data score: 0.7991510708084122\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 4900 ...\n",
      "dev score: 0.9254997842657846 and sample data score: 0.7992282958199356\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 5200 ...\n",
      "dev score: 0.9255770475300209 and sample data score: 0.7994087023587635\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Narrowing down the search for best parameters.\n",
    "deltas = [3700, 4000, 4300, 4600, 4900, 5200]\n",
    "lambda_bigram_vals = [0.1]\n",
    "lambda_trigram_vals = [0.1]\n",
    "\n",
    "for i in range(len(lambda_bigram_vals)):\n",
    "    lambda_bigram = lambda_bigram_vals[i]\n",
    "    lambda_trigram = lambda_trigram_vals[i]\n",
    "    for delta in deltas:\n",
    "        Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * delta**(len(k)))))\n",
    "        Pwc = Pdist_cond(Pw, data=datafile(\"../data/count_2w.txt\", mode='bigram'))\n",
    "        Pwc2 = Pdist_cond_tri(Pwc, data=datafile(\"../data/count_3w.txt\", mode='trigram'))\n",
    "        segmenter = Segment(Pw, Pwc, Pwc2, lambda_bigram=lambda_bigram, lambda_trigram=lambda_trigram)\n",
    "        # evaluate results on dev\n",
    "        output_full = run_input_file(\"../data/input/dev.txt\")\n",
    "        tally_dev = evaluate_input_file('../data/reference/dev.out', output_full)\n",
    "        # evaluate results on the sample sentences from 1 Million Chinese sentences\n",
    "        output_full = run_input_file(\"../data/input/million_sent_sample.txt\")\n",
    "        tally_sample = evaluate_input_file('../data/reference/million_sent_sample.out', output_full)\n",
    "        print('Testing lambda_bigram {}, lambda_trigram {} and delta {} ...'.format(lambda_bigram, lambda_trigram, delta))\n",
    "        print('dev score: {} and sample data score: {}'.format(tally_dev, tally_sample))\n",
    "        print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 5200 ...\n",
      "dev score: 0.9255770475300209 and sample data score: 0.7994087023587635\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 5500 ...\n",
      "dev score: 0.9255212077641984 and sample data score: 0.7996915365336418\n",
      "----------\n",
      "Testing lambda_bigram 0.1, lambda_trigram 0.1 and delta 6000 ...\n",
      "dev score: 0.925343105554358 and sample data score: 0.7985616130482246\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Narrowing down the search for best parameters.\n",
    "deltas = [5200, 5500, 6000]\n",
    "lambda_bigram_vals = [0.1]\n",
    "lambda_trigram_vals = [0.1]\n",
    "\n",
    "for i in range(len(lambda_bigram_vals)):\n",
    "    lambda_bigram = lambda_bigram_vals[i]\n",
    "    lambda_trigram = lambda_trigram_vals[i]\n",
    "    for delta in deltas:\n",
    "        Pw = Pdist(data=datafile(\"../data/count_1w.txt\"), missingfn=(lambda k, N: 10./(N * delta**(len(k)))))\n",
    "        Pwc = Pdist_cond(Pw, data=datafile(\"../data/count_2w.txt\", mode='bigram'))\n",
    "        Pwc2 = Pdist_cond_tri(Pwc, data=datafile(\"../data/count_3w.txt\", mode='trigram'))\n",
    "        segmenter = Segment(Pw, Pwc, Pwc2, lambda_bigram=lambda_bigram, lambda_trigram=lambda_trigram)\n",
    "        # evaluate results on dev\n",
    "        output_full = run_input_file(\"../data/input/dev.txt\")\n",
    "        tally_dev = evaluate_input_file('../data/reference/dev.out', output_full)\n",
    "        # evaluate results on the sample sentences from 1 Million Chinese sentences\n",
    "        output_full = run_input_file(\"../data/input/million_sent_sample.txt\")\n",
    "        tally_sample = evaluate_input_file('../data/reference/million_sent_sample.out', output_full)\n",
    "        print('Testing lambda_bigram {}, lambda_trigram {} and delta {} ...'.format(lambda_bigram, lambda_trigram, delta))\n",
    "        print('dev score: {} and sample data score: {}'.format(tally_dev, tally_sample))\n",
    "        print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above grid search shows that the model gives the highest score when we use the following parameters:<br><br>\n",
    "delta (smoothing parameter for unigram) = 5500<br>\n",
    "lambda for bigram = 0.1<br>\n",
    "lambda for trigram = 0.1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
